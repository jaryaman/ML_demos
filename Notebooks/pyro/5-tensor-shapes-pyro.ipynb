{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor shapes in pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://pytorch.org/docs/master/notes/broadcasting.html) for how broadcasting works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution shapes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest distribution shape is a single univariate distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "from pyro.distributions import Bernoulli, Categorical, Normal, MultivariateNormal\n",
    "from pyro.infer import Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "import pyro.poutine as poutine\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert d.event_shape == ()\n",
    "assert d.batch_shape == ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.shape == ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6931)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.log_prob(x)  # the log likelihood of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert d.log_prob(x).shape == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions can be **batched** by passing in batched parameters. Distributions have two shape attributions\n",
    "- `.batch_shape` = conditionally independent random variables. This can be e.g. the number of IID samples you generate.\n",
    "- `.event_shape` = dependent random variables. E.g. a distribution over scalars has `len(event_shape)==0`, vectors `len(event_shape==1)`, and matrices `len(event_shape==2)`.\n",
    "\n",
    "`.log_prob()` produces a single number for each event, and so has the same shape as `.batch_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(0.5 * torch.ones(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert d.batch_shape == (3,4)\n",
    "assert d.event_shape == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to batch distributions is via `.expand()`. Parameters must be identical along the leftmost dimensions (**Todo: don't understand**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(torch.tensor([0.1, 0.2, 0.3, 0.4])).expand([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert d.batch_shape == (3,4)\n",
    "assert d.event_shape == ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.shape == (3,4)\n",
    "assert d.log_prob(x).shape == (3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Multivariate normal distribution has a non-empty `.event_shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = MultivariateNormal(loc=torch.zeros(3), covariance_matrix=torch.eye(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == (3,)\n",
    "assert len(d.event_shape) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.shape == (3,)\n",
    "assert d.log_prob(x).shape == ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can treat a univariate distribution as multivariate by calling `to_event(n)` where `n` is the number of batch dimensions (from the right) to declare as *dependent*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Bernoulli(0.5).expand([3,4]).to_event(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert d.batch_shape == (3,)\n",
    "assert d.event_shape == (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples have shape `batch_shape + event_shape` whereas `.log_prob(x)` has shape `batch_shape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.shape == (3,4)\n",
    "assert d.log_prob(x).shape == (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to ensure that `batch_shape` is carefully controlled by either trimming it down with `.to_event` or by declaring dimensions as independent via `pyro.plate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is always safe to assume dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often in Pyro we'll declare some dimensions as dependent even though they are in fact independent. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pyro.sample(\"x\", Normal(0,1).expand([10]).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.shape == (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful because:\n",
    "- It allows us to swap in a higher-dimensional RV later\n",
    "- Allows us to simplify the code because we then don't need a plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyro.plate(\"x_plate\", 10):\n",
    "    x = pyro.sample(\"x\", Normal(0,1))  # .expand([10]) is automatic\n",
    "    assert x.shape == (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between these two versions is, with `.to_event`, Pyro assumes that the samples are dependent (even though they are actually independent).\n",
    "\n",
    "This is always a safe thing to do -- like for d-separation, it is always safe to assume variables may be dependent, but unsafe to assume independence because this narrows the model class to outside of the true model.\n",
    "\n",
    "In practice, SVI uses reparametrized gradient estimators for `Normal` so, in this case, both gradient estimators have the same performance.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring independent dimensions with `plate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyro.plate` allows you to declare certain **batch** dimensions as independent, allowing inference algorithms to take advantage of this independence. For example, the index of data over a minibatch, is an independent dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plates declare the **rightmost**  batch dimension as independent. Plates can also be nested.\n",
    "```python\n",
    "with pyro.plate(\"x_axis\", 320):\n",
    "    # within this context, batch dimension -1 is independent\n",
    "    with pyro.plate(\"y_axis\", 200):\n",
    "        # within this context, batch dimensions -2 and -1 are independent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also mix and match plates\n",
    "```python\n",
    "x_axis = pyro.plate(\"x_axis\", 3, dim=-2)\n",
    "y_axis = pyro.plate(\"y_axis\", 2, dim=-3)\n",
    "with x_axis:\n",
    "    # within this context, batch dimension -2 is independent\n",
    "with y_axis:\n",
    "    # within this context, batch dimension -3 is independent\n",
    "with x_axis, y_axis:\n",
    "    # within this context, batch dimensions -3 and -2 are independent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pyro.sample(\"b\", Normal(torch.zeros(2), 1).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert b.shape == (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1():\n",
    "    a = pyro.sample(\"a\", Normal(0,1))\n",
    "    b = pyro.sample(\"b\", Normal(torch.zeros(2), 1).to_event(1)) \n",
    "    with pyro.plate(\"c_plate\", 2):\n",
    "        c = pyro.sample(\"c\", Normal(torch.zeros(2), 1))  \n",
    "    with pyro.plate(\"d_plate\", 3):\n",
    "        d = pyro.sample(\"d\", Normal(torch.zeros(3,4,5), 1).to_event(2))\n",
    "    \n",
    "    assert a.shape == ()       # batch_shape == ()       event_shape == ()\n",
    "    assert b.shape == (2,)     # batch_shape == ()       event_shape == (2,)\n",
    "    assert c.shape == (2,)     # batch_shape == (2,)     event_shape == ()\n",
    "    assert d.shape == (3,4,5)  # batch_shape == (3,)     event_shape == (4,5)\n",
    "    \n",
    "    #----------------\n",
    "    \n",
    "    x_axis = pyro.plate(\"x_axis\", 3, dim=-2)\n",
    "    y_axis = pyro.plate(\"y_axis\", 2, dim=-3)\n",
    "    with x_axis:\n",
    "        x = pyro.sample(\"x\", Normal(0,1))\n",
    "    with y_axis:\n",
    "        y = pyro.sample(\"y\", Normal(0,1))\n",
    "    with x_axis, y_axis:\n",
    "        xy = pyro.sample(\"xy\", Normal(0,1))\n",
    "        z = pyro.sample(\"z\", Normal(0,1).expand([5]).to_event(1))\n",
    "    \n",
    "    assert x.shape  == (3,1)     # batch_shape == (3,1)  event_shape == ()\n",
    "    assert y.shape  == (2,1,1)   # batch_shape == (2,1,1)  event_shape == ()\n",
    "    assert xy.shape == (2,3,1)  # batch_shape == (2,3,1)  event_shape == ()\n",
    "    assert z.shape  == (2,3,1,5)   # batch_shape == (2,3,1)  event_shape == (5,) <- this one is non-intuitive!!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, guide, loss):\n",
    "    pyro.clear_param_store()\n",
    "    loss.loss(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model1, model1, Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example might also be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1, 1, 1, 1, 5, 6])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_axis = pyro.plate(\"x_axis\", 3, dim=-5)\n",
    "y_axis = pyro.plate(\"y_axis\", 2, dim=-6)\n",
    "with x_axis, y_axis:        \n",
    "    z = pyro.sample(\"z\", Normal(0,1).expand([5,6]).to_event(2))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can programatically observe the shapes of all objects in a model with `trace.format_shapes()`, printing\n",
    "1. The distribution shpae\n",
    "2. The value shape\n",
    "3. The log probability shape (if calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace Shapes:            \n",
      " Param Sites:            \n",
      "Sample Sites:            \n",
      "       a dist       |    \n",
      "        value       |    \n",
      "       b dist       | 2  \n",
      "        value       | 2  \n",
      " c_plate dist       |    \n",
      "        value     2 |    \n",
      "       c dist     2 |    \n",
      "        value     2 |    \n",
      " d_plate dist       |    \n",
      "        value     3 |    \n",
      "       d dist     3 | 4 5\n",
      "        value     3 | 4 5\n",
      "  x_axis dist       |    \n",
      "        value     3 |    \n",
      "  y_axis dist       |    \n",
      "        value     2 |    \n",
      "       x dist   3 1 |    \n",
      "        value   3 1 |    \n",
      "       y dist 2 1 1 |    \n",
      "        value 2 1 1 |    \n",
      "      xy dist 2 3 1 |    \n",
      "        value 2 3 1 |    \n",
      "       z dist 2 3 1 | 5  \n",
      "        value 2 3 1 | 5  \n"
     ]
    }
   ],
   "source": [
    "trace = poutine.trace(model1).get_trace()\n",
    "# trace.compute_log_prob()  # <- optional\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling tensors inside a `plate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main uses of `plate` is to subsample data. Since data are conditionally independent inside a plate, the expected value of loss on e.g. half the data should be half the expected loss on the full data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subsample data, pyro needs to know both the original data size and the subsample size. Pyro then chooses a random subset of data and yield a set of indicies (although this is customizable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.arange(100.)\n",
    "\n",
    "def model2():\n",
    "    mean = pyro.param(\"mean\", torch.zeros(len(data)))\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=10) as ind:\n",
    "        assert len(ind) == 10\n",
    "        batch = data[ind]        \n",
    "        mean_batch = mean[ind]\n",
    "        \n",
    "        # do stuff with the batch\n",
    "        x = pyro.sample(\"x\", Normal(mean_batch, 1), obs=batch)\n",
    "        assert len(x) == 10\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model2, guide=lambda: None, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting to allow parallel enumeration of discrete latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyro can enumerate discrete latent variables in parallel. To do this, Pyro needs to pre-allocate tensor dimensions that it can use for enumeration. To avoid conflicting with other dimensions we want to use for plates, we need to declare a budget for the maximum number of tensor dimensions we will use for parallel enumeration: `max_plate_nesting`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_plate_nesting` = budget for maximum number of tensor dimensions to be used for parallel enumeration. (Can often be auto-inferred, except for dyanamic model structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3():\n",
    "    p = pyro.param(\"p\", torch.arange(6.)/6)\n",
    "    locs = pyro.param(\"locs\", torch.tensor([-1., 1.]))\n",
    "    \n",
    "    a = pyro.sample(\"a\", Categorical(torch.ones(6)/6))\n",
    "    b = pyro.sample(\"b\", Bernoulli(p[a]))  # note this depends on a\n",
    "    with pyro.plate(\"c_plate\", 4):\n",
    "        c = pyro.sample(\"c\", Bernoulli(0.3))\n",
    "        with pyro.plate(\"d_plate\", 5):\n",
    "            d = pyro.sample(\"d\", Bernoulli(0.4))\n",
    "            e_loc = locs[d.long()].unsqueeze(-1)\n",
    "            e_scale = torch.arange(1., 8.)\n",
    "            e = pyro.sample(\"e\", Normal(e_loc, e_scale).to_event(1)) # note this depends on d\n",
    "    \n",
    "    assert a.shape == () \n",
    "    assert b.shape == () \n",
    "    assert c.shape == (4,)\n",
    "    assert d.shape == (5,4)        \n",
    "\n",
    "    assert e_loc.shape   == (5,4,1)  # extra dim for the unsqueeze\n",
    "    assert e_scale.shape == (7,)\n",
    "    \n",
    "    assert e.shape == (5,4,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model3, model3, Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the `@config_enumerate` decorator and specify that `max_plate_nesting=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config_enumerate\n",
    "def model3():\n",
    "    p = pyro.param(\"p\", torch.arange(6.)/6)\n",
    "    locs = pyro.param(\"locs\", torch.tensor([-1., 1.]))\n",
    "    \n",
    "    a = pyro.sample(\"a\", Categorical(torch.ones(6)/6))  # we now enumerate this discrete latent variable, dim=6\n",
    "    b = pyro.sample(\"b\", Bernoulli(p[a]))  # we enumerate this too, and it has dim=2\n",
    "    with pyro.plate(\"c_plate\", 4):\n",
    "        c = pyro.sample(\"c\", Bernoulli(0.3))\n",
    "        with pyro.plate(\"d_plate\", 5):\n",
    "            d = pyro.sample(\"d\", Bernoulli(0.4))\n",
    "            e_loc = locs[d.long()].unsqueeze(-1)\n",
    "            e_scale = torch.arange(1., 8.)\n",
    "            e = pyro.sample(\"e\", Normal(e_loc, e_scale).to_event(1)) # note this depends on d\n",
    "    \n",
    "    #                         enumerated|batch|event dims\n",
    "    assert a.shape ==        (         6, 1, 1   ) # because max_plate_nesting=2, we get two extra batch dims. pyro allocates the dim=-3\n",
    "    assert b.shape ==        (      2, 1, 1, 1   ) # pyro allocates to dim=-4\n",
    "    assert c.shape ==        (   2, 1, 1, 1, 1   ) # pyro allocates to dim=-5 \n",
    "    assert d.shape ==        (2, 1, 1, 1, 1, 1   ) # pyro allocates to dim=-6\n",
    "    \n",
    "    assert e_loc.shape   ==  (2, 1, 1, 1, 1, 1, 1,)\n",
    "    assert e_scale.shape ==  (                  7,)\n",
    "    \n",
    "    assert e.shape ==        (2, 1, 1, 1, 5, 4, 7)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Don't fully understand `e`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model3, model3, TraceEnum_ELBO(max_plate_nesting=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "     max_plate_nesting = 2\n",
    "            |<->|\n",
    "enumeration batch event\n",
    "------------|---|-----\n",
    "           6|1 1|     a = pyro.sample(\"a\", Categorical(torch.ones(6) / 6))\n",
    "         2 1|1 1|     b = pyro.sample(\"b\", Bernoulli(p[a]))\n",
    "            |   |     with pyro.plate(\"c_plate\", 4):\n",
    "       2 1 1|1 1|         c = pyro.sample(\"c\", Bernoulli(0.3))\n",
    "            |   |         with pyro.plate(\"d_plate\", 5):\n",
    "     2 1 1 1|1 1|             d = pyro.sample(\"d\", Bernoulli(0.4))\n",
    "     2 1 1 1|1 1|1            e_loc = locs[d.long()].unsqueeze(-1)\n",
    "            |   |7            e_scale = torch.arange(1., 8.)\n",
    "     2 1 1 1|5 4|7            e = pyro.sample(\"e\", Normal(e_loc, e_scale)\n",
    "            |   |                             .to_event(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following code to examine the model with enumeration semantics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace Shapes:                \n",
      " Param Sites:                \n",
      "            p             6  \n",
      "         locs             2  \n",
      "Sample Sites:                \n",
      "       a dist             |  \n",
      "        value       6 1 1 |  \n",
      "     log_prob       6 1 1 |  \n",
      "       b dist       6 1 1 |  \n",
      "        value     2 1 1 1 |  \n",
      "     log_prob     2 6 1 1 |  \n",
      " c_plate dist             |  \n",
      "        value           4 |  \n",
      "     log_prob             |  \n",
      "       c dist           4 |  \n",
      "        value   2 1 1 1 1 |  \n",
      "     log_prob   2 1 1 1 4 |  \n",
      " d_plate dist             |  \n",
      "        value           5 |  \n",
      "     log_prob             |  \n",
      "       d dist         5 4 |  \n",
      "        value 2 1 1 1 1 1 |  \n",
      "     log_prob 2 1 1 1 5 4 |  \n",
      "       e dist 2 1 1 1 5 4 | 7\n",
      "        value 2 1 1 1 5 4 | 7\n",
      "     log_prob 2 1 1 1 5 4 |  \n"
     ]
    }
   ],
   "source": [
    "trace = poutine.trace(poutine.enum(model3, first_available_dim=-3)).get_trace()\n",
    "trace.compute_log_prob()  # optional, but allows printing of log_prob shapes\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing parallelizable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two notable tricks: **broadcasting** and **ellipsis slicing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "from pyro.distributions.util import broadcast_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code should work both with and without enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 8\n",
    "height = 10\n",
    "sparse_pixels = torch.LongTensor([[3,2], [3, 5], [3, 9], [7, 1]])\n",
    "enumerated = None\n",
    "\n",
    "def fun(observe):\n",
    "    p_x = pyro.param(\"p_x\", torch.tensor(0.1), constraint=constraints.unit_interval)\n",
    "    p_y = pyro.param(\"p_x\", torch.tensor(0.1), constraint=constraints.unit_interval)\n",
    "    x_axis = pyro.plate(\"x_axis\", width, dim=-2)\n",
    "    y_axis = pyro.plate(\"y_axis\", height, dim=-1)\n",
    "    \n",
    "    with x_axis:\n",
    "        x_active = pyro.sample(\"x_active\", Bernoulli(p_x))\n",
    "    with y_axis:\n",
    "        y_active = pyro.sample(\"y_active\", Bernoulli(p_y))\n",
    "    if enumerated:\n",
    "        assert x_active.shape == (2,1,1)\n",
    "        assert y_active.shape == (2,1,1,1)\n",
    "    else:\n",
    "        assert x_active.shape == (width, 1)\n",
    "        assert y_active.shape == (height, )\n",
    "        \n",
    "    # Trick: Apply broadcasting\n",
    "    p = 0.1 + 0.5 * x_active * y_active  # shape is (width, height) or (2,2,1,1)\n",
    "    if enumerated:\n",
    "        assert p.shape == (2, 2, 1, 1)\n",
    "    else:\n",
    "        assert p.shape == (width, height)\n",
    "    \n",
    "    dense_pixels = p.new_zeros(broadcast_shape(p.shape, (width, height))) # TODO: ?\n",
    "    \n",
    "    # Trick: Use ellipsis slicing\n",
    "    # This allows Pyro to add arbitrary dimensions on the left\n",
    "    for x, y in sparse_pixels:\n",
    "        dense_pixels[..., x, y] = 1\n",
    "    if enumerated:\n",
    "        assert dense_pixels.shape == (2, 2, width, height)\n",
    "    else:\n",
    "        assert dense_pixels.shape == (width, height)\n",
    "    \n",
    "    with x_axis, y_axis:\n",
    "        if observe:\n",
    "            pyro.sample(\"pixels\", Bernoulli(p), obs=dense_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4():\n",
    "    fun(observe=True)\n",
    "    \n",
    "def guide4():\n",
    "    fun(observe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without enumeration\n",
    "enumerated = False\n",
    "test_model(model4, guide4, Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With enumeration\n",
    "enumerated = True\n",
    "test_model(model4, config_enumerate(guide4, \"parallel\"), TraceEnum_ELBO(max_plate_nesting=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
